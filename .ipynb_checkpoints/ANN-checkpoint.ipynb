{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - 50_startups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"50_startups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>165349.2</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>162597.7</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0   165349.2       136897.80        471784.10    New York  192261.83\n",
       "1   162597.7       151377.59        443898.53  California  191792.06"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R&D Spend          False\n",
       "Administration     False\n",
       "Marketing Spend    False\n",
       "State              False\n",
       "Profit             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "dataset['State'] = le.fit_transform(dataset['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>2</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>0</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>1</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>2</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>1</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend  State     Profit\n",
       "0  165349.20       136897.80        471784.10      2  192261.83\n",
       "1  162597.70       151377.59        443898.53      0  191792.06\n",
       "2  153441.51       101145.55        407934.54      1  191050.39\n",
       "3  144372.41       118671.85        383199.62      2  182901.99\n",
       "4  142107.34        91391.77        366168.42      1  166187.94"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = dataset.iloc[:,0:4].values\n",
    "y = dataset.iloc[:,4:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "z=one.fit_transform(x[:,3:4]).toarray()\n",
    "x=np.delete(x,3,axis=1)\n",
    "x=np.concatenate((z,x),axis=1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one=OneHotEncoder()\n",
    "z=one.fit_transform(x[:,3:4]).toarray()\n",
    "\n",
    "x=np.delete(x,3,axis=1)\n",
    "x=np.concatenate((z,x),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=6, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(Dense(units = 6,init = 'random_uniform',activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=12, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(Dense(units = 12,init = 'random_uniform',activation = 'relu')) # 1st hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=8, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(Dense(units = 8,init = 'random_uniform',activation = 'relu')) # second hiden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=9, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(Dense(units = 9,init = 'random_uniform',activation = 'relu')) # third Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(Dense(units = 1,init = 'random_uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.compile (optimizer = 'adam',loss = 'mse',metrics = ['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "40/40 [==============================] - 0s 12ms/step - loss: 13615988224.0000 - mse: 13615987712.0000\n",
      "Epoch 2/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 13615964160.0000 - mse: 13615964160.0000\n",
      "Epoch 3/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 13615906048.0000 - mse: 13615906816.0000\n",
      "Epoch 4/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 13615774720.0000 - mse: 13615773696.0000\n",
      "Epoch 5/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 13615490048.0000 - mse: 13615490048.0000\n",
      "Epoch 6/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 13614923008.0000 - mse: 13614923776.0000\n",
      "Epoch 7/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 13613755136.0000 - mse: 13613755392.0000\n",
      "Epoch 8/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 13611490560.0000 - mse: 13611489280.0000\n",
      "Epoch 9/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 13607780096.0000 - mse: 13607780352.0000\n",
      "Epoch 10/300\n",
      "40/40 [==============================] - 0s 317us/step - loss: 13600094208.0000 - mse: 13600094208.0000\n",
      "Epoch 11/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 13586578944.0000 - mse: 13586579456.0000\n",
      "Epoch 12/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 13562258944.0000 - mse: 13562259456.0000\n",
      "Epoch 13/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 13528420096.0000 - mse: 13528419328.0000\n",
      "Epoch 14/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 13472518656.0000 - mse: 13472519168.0000\n",
      "Epoch 15/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 13383574528.0000 - mse: 13383574528.0000\n",
      "Epoch 16/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 13248240384.0000 - mse: 13248240640.0000\n",
      "Epoch 17/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 13055216640.0000 - mse: 13055215616.0000\n",
      "Epoch 18/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 12809180160.0000 - mse: 12809180160.0000\n",
      "Epoch 19/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 12856074240.0000 - mse: 12856074240.000 - 0s 268us/step - loss: 12413502720.0000 - mse: 12413503488.0000\n",
      "Epoch 20/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 11923173376.0000 - mse: 11923172352.0000\n",
      "Epoch 21/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 11249673728.0000 - mse: 11249674240.0000\n",
      "Epoch 22/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 10358199680.0000 - mse: 10358200320.0000\n",
      "Epoch 23/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 9261650944.0000 - mse: 9261650944.0000\n",
      "Epoch 24/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 7978050304.0000 - mse: 7978050560.0000\n",
      "Epoch 25/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 6400931200.0000 - mse: 6400930816.0000\n",
      "Epoch 26/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 4848008576.0000 - mse: 4848008704.0000\n",
      "Epoch 27/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 3177569952.0000 - mse: 3177569792.0000\n",
      "Epoch 28/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 1663976160.0000 - mse: 1663976192.0000\n",
      "Epoch 29/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 749165376.0000 - mse: 749165312.0000\n",
      "Epoch 30/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 413230436.0000 - mse: 413230432.0000\n",
      "Epoch 31/300\n",
      "40/40 [==============================] - 0s 121us/step - loss: 589018808.0000 - mse: 589018816.0000\n",
      "Epoch 32/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 708465584.0000 - mse: 708465536.0000\n",
      "Epoch 33/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 626332528.0000 - mse: 626332544.0000\n",
      "Epoch 34/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 467129232.0000 - mse: 467129248.0000\n",
      "Epoch 35/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 399437212.0000 - mse: 399437216.0000\n",
      "Epoch 36/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 401738292.0000 - mse: 401738304.0000\n",
      "Epoch 37/300\n",
      "40/40 [==============================] - 0s 196us/step - loss: 413703894.0000 - mse: 413703872.0000\n",
      "Epoch 38/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 407364788.0000 - mse: 407364800.0000\n",
      "Epoch 39/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 394452612.0000 - mse: 394452608.0000\n",
      "Epoch 40/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 377801652.0000 - mse: 377801664.0000\n",
      "Epoch 41/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 374615580.0000 - mse: 374615616.0000\n",
      "Epoch 42/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 371002296.0000 - mse: 371002304.0000\n",
      "Epoch 43/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 369775332.0000 - mse: 369775296.0000\n",
      "Epoch 44/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 367459732.0000 - mse: 367459744.0000\n",
      "Epoch 45/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 363405440.0000 - mse: 363405440.0000\n",
      "Epoch 46/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 358570216.0000 - mse: 358570208.0000\n",
      "Epoch 47/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 356889588.0000 - mse: 356889600.0000\n",
      "Epoch 48/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 355608440.0000 - mse: 355608448.0000\n",
      "Epoch 49/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 351157186.0000 - mse: 351157184.0000\n",
      "Epoch 50/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 348329172.0000 - mse: 348329152.0000\n",
      "Epoch 51/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 347725432.0000 - mse: 347725440.0000\n",
      "Epoch 52/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 342654192.0000 - mse: 342654208.0000\n",
      "Epoch 53/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 338973916.0000 - mse: 338973888.0000\n",
      "Epoch 54/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 337708292.0000 - mse: 337708288.0000\n",
      "Epoch 55/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 333811832.0000 - mse: 333811808.0000\n",
      "Epoch 56/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 332570520.0000 - mse: 332570528.0000\n",
      "Epoch 57/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 330227974.0000 - mse: 330227968.0000\n",
      "Epoch 58/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 326098732.0000 - mse: 326098752.0000\n",
      "Epoch 59/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 67930896.0000 - mse: 67930896.000 - 0s 122us/step - loss: 324062104.0000 - mse: 324062112.0000\n",
      "Epoch 60/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 320733354.0000 - mse: 320733376.0000\n",
      "Epoch 61/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 318788308.0000 - mse: 318788288.0000\n",
      "Epoch 62/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 315751948.0000 - mse: 315751936.0000\n",
      "Epoch 63/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 313954340.0000 - mse: 313954336.0000\n",
      "Epoch 64/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 310946400.0000 - mse: 310946400.0000\n",
      "Epoch 65/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 507292768.0000 - mse: 507292768.000 - 0s 122us/step - loss: 308934900.0000 - mse: 308934912.0000\n",
      "Epoch 66/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 306008428.0000 - mse: 306008384.0000\n",
      "Epoch 67/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 302839672.0000 - mse: 302839712.0000\n",
      "Epoch 68/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 300474844.0000 - mse: 300474816.0000\n",
      "Epoch 69/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 300805204.0000 - mse: 300805184.0000\n",
      "Epoch 70/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 296932978.0000 - mse: 296932960.0000\n",
      "Epoch 71/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 294014854.0000 - mse: 294014848.0000\n",
      "Epoch 72/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 292775180.0000 - mse: 292775200.0000\n",
      "Epoch 73/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 289933898.0000 - mse: 289933888.0000\n",
      "Epoch 74/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 286413024.0000 - mse: 286413024.0000\n",
      "Epoch 75/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 284641134.0000 - mse: 284641120.0000\n",
      "Epoch 76/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 283363724.0000 - mse: 283363712.0000\n",
      "Epoch 77/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 280568380.0000 - mse: 280568384.0000\n",
      "Epoch 78/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 279583688.0000 - mse: 279583680.0000\n",
      "Epoch 79/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 275632396.0000 - mse: 275632384.0000\n",
      "Epoch 80/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 274056688.0000 - mse: 274056672.0000\n",
      "Epoch 81/300\n",
      "40/40 [==============================] - 0s 269us/step - loss: 273032664.0000 - mse: 273032640.0000\n",
      "Epoch 82/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 270003068.0000 - mse: 270003072.0000\n",
      "Epoch 83/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 267982196.0000 - mse: 267982176.0000\n",
      "Epoch 84/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 267008858.0000 - mse: 267008848.0000\n",
      "Epoch 85/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 263720112.0000 - mse: 263720096.0000\n",
      "Epoch 86/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 262041286.0000 - mse: 262041296.0000\n",
      "Epoch 87/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 261716164.0000 - mse: 261716144.0000\n",
      "Epoch 88/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 258592850.0000 - mse: 258592848.0000\n",
      "Epoch 89/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 257489892.0000 - mse: 257489888.0000\n",
      "Epoch 90/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 254204436.0000 - mse: 254204416.0000\n",
      "Epoch 91/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 252132460.0000 - mse: 252132448.0000\n",
      "Epoch 92/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 254265900.0000 - mse: 254265904.0000\n",
      "Epoch 93/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 249456952.0000 - mse: 249456976.0000\n",
      "Epoch 94/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 247182376.0000 - mse: 247182384.0000\n",
      "Epoch 95/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 245225454.0000 - mse: 245225440.0000\n",
      "Epoch 96/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 243718862.0000 - mse: 243718864.0000\n",
      "Epoch 97/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 242443700.0000 - mse: 242443696.0000\n",
      "Epoch 98/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 239978050.0000 - mse: 239978032.0000\n",
      "Epoch 99/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 241487168.0000 - mse: 241487184.0000\n",
      "Epoch 100/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 236901132.0000 - mse: 236901152.0000\n",
      "Epoch 101/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 235633418.0000 - mse: 235633408.0000\n",
      "Epoch 102/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 233040488.0000 - mse: 233040480.0000\n",
      "Epoch 103/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 233841558.0000 - mse: 233841568.0000\n",
      "Epoch 104/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 232254116.0000 - mse: 232254112.0000\n",
      "Epoch 105/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 229565120.0000 - mse: 229565136.0000\n",
      "Epoch 106/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 229275612.0000 - mse: 229275600.0000\n",
      "Epoch 107/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 226405308.0000 - mse: 226405328.0000\n",
      "Epoch 108/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 224354666.0000 - mse: 224354656.0000\n",
      "Epoch 109/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 223591092.0000 - mse: 223591088.0000\n",
      "Epoch 110/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 222179254.0000 - mse: 222179248.0000\n",
      "Epoch 111/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 220901040.0000 - mse: 220901040.0000\n",
      "Epoch 112/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 219118984.0000 - mse: 219118976.0000\n",
      "Epoch 113/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 217751128.0000 - mse: 217751120.0000\n",
      "Epoch 114/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 216968312.0000 - mse: 216968320.0000\n",
      "Epoch 115/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 215426148.0000 - mse: 215426144.0000\n",
      "Epoch 116/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 213970904.0000 - mse: 213970912.0000\n",
      "Epoch 117/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 212800952.0000 - mse: 212800944.0000\n",
      "Epoch 118/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 211830718.0000 - mse: 211830704.0000\n",
      "Epoch 119/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 210053606.0000 - mse: 210053600.0000\n",
      "Epoch 120/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 209100426.0000 - mse: 209100432.0000\n",
      "Epoch 121/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 208155065.0000 - mse: 208155072.0000\n",
      "Epoch 122/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 207135512.0000 - mse: 207135520.0000\n",
      "Epoch 123/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 205495818.0000 - mse: 205495840.0000\n",
      "Epoch 124/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 204477286.0000 - mse: 204477280.0000\n",
      "Epoch 125/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 203897036.0000 - mse: 203897040.0000\n",
      "Epoch 126/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 201740396.0000 - mse: 201740416.0000\n",
      "Epoch 127/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 200816818.0000 - mse: 200816816.0000\n",
      "Epoch 128/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 201641078.0000 - mse: 201641072.0000\n",
      "Epoch 129/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 199297612.0000 - mse: 199297616.0000\n",
      "Epoch 130/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 197924848.0000 - mse: 197924864.0000\n",
      "Epoch 131/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 196799880.0000 - mse: 196799888.0000\n",
      "Epoch 132/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 195659380.0000 - mse: 195659376.0000\n",
      "Epoch 133/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 195063612.0000 - mse: 195063600.0000\n",
      "Epoch 134/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 193935612.0000 - mse: 193935616.0000\n",
      "Epoch 135/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 194668226.0000 - mse: 194668208.0000\n",
      "Epoch 136/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 192702370.0000 - mse: 192702368.0000\n",
      "Epoch 137/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 190595279.0000 - mse: 190595280.0000\n",
      "Epoch 138/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 190976916.0000 - mse: 190976928.0000\n",
      "Epoch 139/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 189791612.0000 - mse: 189791616.0000\n",
      "Epoch 140/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 189211846.0000 - mse: 189211840.0000\n",
      "Epoch 141/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 187181028.0000 - mse: 187181024.0000\n",
      "Epoch 142/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 186774060.0000 - mse: 186774064.0000\n",
      "Epoch 143/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 146us/step - loss: 186030852.0000 - mse: 186030848.0000\n",
      "Epoch 144/300\n",
      "40/40 [==============================] - 0s 147us/step - loss: 186018288.0000 - mse: 186018288.0000\n",
      "Epoch 145/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 184294638.0000 - mse: 184294624.0000\n",
      "Epoch 146/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 184798052.0000 - mse: 184798048.0000\n",
      "Epoch 147/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 182646973.0000 - mse: 182646960.0000\n",
      "Epoch 148/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 181793294.0000 - mse: 181793296.0000\n",
      "Epoch 149/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 180980586.0000 - mse: 180980576.0000\n",
      "Epoch 150/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 181469642.0000 - mse: 181469648.0000\n",
      "Epoch 151/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 180421948.0000 - mse: 180421936.0000\n",
      "Epoch 152/300\n",
      "40/40 [==============================] - 0s 147us/step - loss: 178916844.0000 - mse: 178916832.0000\n",
      "Epoch 153/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 178682628.0000 - mse: 178682624.0000\n",
      "Epoch 154/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 177783828.0000 - mse: 177783840.0000\n",
      "Epoch 155/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 177867396.0000 - mse: 177867392.0000\n",
      "Epoch 156/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 176839824.0000 - mse: 176839824.0000\n",
      "Epoch 157/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 176031496.0000 - mse: 176031488.0000\n",
      "Epoch 158/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 174928104.0000 - mse: 174928096.0000\n",
      "Epoch 159/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 174482416.0000 - mse: 174482416.0000\n",
      "Epoch 160/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 173717842.0000 - mse: 173717856.0000\n",
      "Epoch 161/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 173580546.0000 - mse: 173580560.0000\n",
      "Epoch 162/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 172912086.0000 - mse: 172912096.0000\n",
      "Epoch 163/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 172483294.0000 - mse: 172483296.0000\n",
      "Epoch 164/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 172239306.0000 - mse: 172239312.0000\n",
      "Epoch 165/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 171393390.0000 - mse: 171393392.0000\n",
      "Epoch 166/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 170971533.0000 - mse: 170971552.0000\n",
      "Epoch 167/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 170448482.0000 - mse: 170448480.0000\n",
      "Epoch 168/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 170215644.0000 - mse: 170215648.0000\n",
      "Epoch 169/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 168596401.0000 - mse: 168596384.0000\n",
      "Epoch 170/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 167989328.0000 - mse: 167989328.0000\n",
      "Epoch 171/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 168223950.0000 - mse: 168223952.0000\n",
      "Epoch 172/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 167946316.0000 - mse: 167946320.0000\n",
      "Epoch 173/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 167277334.0000 - mse: 167277344.0000\n",
      "Epoch 174/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 166246765.0000 - mse: 166246768.0000\n",
      "Epoch 175/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 166430490.0000 - mse: 166430496.0000\n",
      "Epoch 176/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 166030842.0000 - mse: 166030848.0000\n",
      "Epoch 177/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 165165336.0000 - mse: 165165344.0000\n",
      "Epoch 178/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 165322516.0000 - mse: 165322512.0000\n",
      "Epoch 179/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 268242816.0000 - mse: 268242816.000 - 0s 146us/step - loss: 164570900.0000 - mse: 164570912.0000\n",
      "Epoch 180/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 163874474.0000 - mse: 163874480.0000\n",
      "Epoch 181/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 163222674.0000 - mse: 163222688.0000\n",
      "Epoch 182/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 163201004.0000 - mse: 163200992.0000\n",
      "Epoch 183/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 162710432.0000 - mse: 162710432.0000\n",
      "Epoch 184/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 162147018.0000 - mse: 162147008.0000\n",
      "Epoch 185/300\n",
      "40/40 [==============================] - ETA: 0s - loss: 143428464.0000 - mse: 143428464.000 - 0s 146us/step - loss: 162351220.0000 - mse: 162351200.0000\n",
      "Epoch 186/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 162821436.0000 - mse: 162821440.0000\n",
      "Epoch 187/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 161416608.0000 - mse: 161416608.0000\n",
      "Epoch 188/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 161331786.0000 - mse: 161331792.0000\n",
      "Epoch 189/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 160980852.0000 - mse: 160980848.0000\n",
      "Epoch 190/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 160786710.0000 - mse: 160786720.0000\n",
      "Epoch 191/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 160147950.0000 - mse: 160147936.0000\n",
      "Epoch 192/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 159760742.0000 - mse: 159760736.0000\n",
      "Epoch 193/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 159982250.0000 - mse: 159982240.0000\n",
      "Epoch 194/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 160050772.0000 - mse: 160050768.0000\n",
      "Epoch 195/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 158811546.0000 - mse: 158811536.0000\n",
      "Epoch 196/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 159047164.0000 - mse: 159047152.0000\n",
      "Epoch 197/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 158862038.0000 - mse: 158862048.0000\n",
      "Epoch 198/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 158214402.0000 - mse: 158214400.0000\n",
      "Epoch 199/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 158237270.0000 - mse: 158237264.0000\n",
      "Epoch 200/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 157755552.0000 - mse: 157755552.0000\n",
      "Epoch 201/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 156955416.0000 - mse: 156955424.0000\n",
      "Epoch 202/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 156678134.0000 - mse: 156678144.0000\n",
      "Epoch 203/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 156590836.0000 - mse: 156590832.0000\n",
      "Epoch 204/300\n",
      "40/40 [==============================] - 0s 147us/step - loss: 156547138.0000 - mse: 156547152.0000\n",
      "Epoch 205/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 156211642.0000 - mse: 156211632.0000\n",
      "Epoch 206/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155949626.0000 - mse: 155949616.0000\n",
      "Epoch 207/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 156198242.0000 - mse: 156198240.0000\n",
      "Epoch 208/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 156300932.0000 - mse: 156300928.0000\n",
      "Epoch 209/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 156352794.0000 - mse: 156352800.0000\n",
      "Epoch 210/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155419962.0000 - mse: 155419952.0000\n",
      "Epoch 211/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 155373576.0000 - mse: 155373584.0000\n",
      "Epoch 212/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155809138.0000 - mse: 155809136.0000\n",
      "Epoch 213/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 155204640.0000 - mse: 155204640.0000\n",
      "Epoch 214/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 171us/step - loss: 154634426.0000 - mse: 154634416.0000\n",
      "Epoch 215/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155262484.0000 - mse: 155262496.0000\n",
      "Epoch 216/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 154508888.0000 - mse: 154508896.0000\n",
      "Epoch 217/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 154557992.0000 - mse: 154557984.0000\n",
      "Epoch 218/300\n",
      "40/40 [==============================] - 0s 683us/step - loss: 154183156.0000 - mse: 154183168.0000\n",
      "Epoch 219/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 155238432.0000 - mse: 155238432.0000\n",
      "Epoch 220/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155062952.0000 - mse: 155062960.0000\n",
      "Epoch 221/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 154899204.0000 - mse: 154899200.0000\n",
      "Epoch 222/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 153699854.0000 - mse: 153699856.0000\n",
      "Epoch 223/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 153274824.0000 - mse: 153274832.0000\n",
      "Epoch 224/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 155078346.0000 - mse: 155078352.0000\n",
      "Epoch 225/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 153550316.0000 - mse: 153550304.0000\n",
      "Epoch 226/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 153597956.0000 - mse: 153597952.0000\n",
      "Epoch 227/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 152906552.0000 - mse: 152906544.0000\n",
      "Epoch 228/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 152910092.0000 - mse: 152910096.0000\n",
      "Epoch 229/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 152127897.0000 - mse: 152127904.0000\n",
      "Epoch 230/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 152168874.0000 - mse: 152168880.0000\n",
      "Epoch 231/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 152380238.0000 - mse: 152380240.0000\n",
      "Epoch 232/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 153905382.0000 - mse: 153905376.0000\n",
      "Epoch 233/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 152239938.0000 - mse: 152239952.0000\n",
      "Epoch 234/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151387250.0000 - mse: 151387248.0000\n",
      "Epoch 235/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 152924178.0000 - mse: 152924192.0000\n",
      "Epoch 236/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 153611046.0000 - mse: 153611040.0000\n",
      "Epoch 237/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151451926.0000 - mse: 151451936.0000\n",
      "Epoch 238/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 153051166.0000 - mse: 153051168.0000\n",
      "Epoch 239/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 153346318.0000 - mse: 153346336.0000\n",
      "Epoch 240/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 151816792.0000 - mse: 151816800.0000\n",
      "Epoch 241/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151910444.0000 - mse: 151910432.0000\n",
      "Epoch 242/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 152439294.0000 - mse: 152439296.0000\n",
      "Epoch 243/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 151937200.0000 - mse: 151937200.0000\n",
      "Epoch 244/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 153709446.0000 - mse: 153709440.0000\n",
      "Epoch 245/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 151426308.0000 - mse: 151426304.0000\n",
      "Epoch 246/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 150986535.0000 - mse: 150986544.0000\n",
      "Epoch 247/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 150869219.0000 - mse: 150869216.0000\n",
      "Epoch 248/300\n",
      "40/40 [==============================] - 0s 244us/step - loss: 151063224.0000 - mse: 151063216.0000\n",
      "Epoch 249/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 150845760.0000 - mse: 150845776.0000\n",
      "Epoch 250/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 151294040.0000 - mse: 151294032.0000\n",
      "Epoch 251/300\n",
      "40/40 [==============================] - 0s 147us/step - loss: 151259382.0000 - mse: 151259376.0000\n",
      "Epoch 252/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150533610.0000 - mse: 150533600.0000\n",
      "Epoch 253/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151823986.0000 - mse: 151823984.0000\n",
      "Epoch 254/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150393068.0000 - mse: 150393056.0000\n",
      "Epoch 255/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151609868.0000 - mse: 151609872.0000\n",
      "Epoch 256/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151386582.0000 - mse: 151386576.0000\n",
      "Epoch 257/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151326142.0000 - mse: 151326128.0000\n",
      "Epoch 258/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151283438.0000 - mse: 151283440.0000\n",
      "Epoch 259/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150499556.0000 - mse: 150499552.0000\n",
      "Epoch 260/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 150514044.0000 - mse: 150514048.0000\n",
      "Epoch 261/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 150996954.0000 - mse: 150996960.0000\n",
      "Epoch 262/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151030516.0000 - mse: 151030512.0000\n",
      "Epoch 263/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150203088.0000 - mse: 150203088.0000\n",
      "Epoch 264/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150495829.0000 - mse: 150495824.0000\n",
      "Epoch 265/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 152519014.0000 - mse: 152519008.0000\n",
      "Epoch 266/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 151526058.0000 - mse: 151526064.0000\n",
      "Epoch 267/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149313518.0000 - mse: 149313520.0000\n",
      "Epoch 268/300\n",
      "40/40 [==============================] - 0s 122us/step - loss: 150172310.0000 - mse: 150172320.0000\n",
      "Epoch 269/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151981192.0000 - mse: 151981200.0000\n",
      "Epoch 270/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 156572997.0000 - mse: 156572992.0000\n",
      "Epoch 271/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 150400372.0000 - mse: 150400352.0000\n",
      "Epoch 272/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149880018.0000 - mse: 149880016.0000\n",
      "Epoch 273/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 150205288.0000 - mse: 150205280.0000\n",
      "Epoch 274/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 150435026.0000 - mse: 150435024.0000\n",
      "Epoch 275/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 151410656.0000 - mse: 151410656.0000\n",
      "Epoch 276/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150397638.0000 - mse: 150397648.0000\n",
      "Epoch 277/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 150091032.0000 - mse: 150091040.0000\n",
      "Epoch 278/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151132276.0000 - mse: 151132272.0000\n",
      "Epoch 279/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149899055.0000 - mse: 149899056.0000\n",
      "Epoch 280/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 150096696.0000 - mse: 150096688.0000\n",
      "Epoch 281/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 149849330.0000 - mse: 149849312.0000\n",
      "Epoch 282/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 151995920.0000 - mse: 151995920.0000\n",
      "Epoch 283/300\n",
      "40/40 [==============================] - 0s 293us/step - loss: 152862908.0000 - mse: 152862896.0000\n",
      "Epoch 284/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149282468.0000 - mse: 149282464.0000\n",
      "Epoch 285/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 153282166.0000 - mse: 153282160.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 151343548.0000 - mse: 151343552.0000\n",
      "Epoch 287/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 149570070.0000 - mse: 149570080.0000\n",
      "Epoch 288/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149644712.0000 - mse: 149644704.0000\n",
      "Epoch 289/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150260046.0000 - mse: 150260048.0000\n",
      "Epoch 290/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 150902814.0000 - mse: 150902816.0000\n",
      "Epoch 291/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 150331396.0000 - mse: 150331392.0000\n",
      "Epoch 292/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 149824232.0000 - mse: 149824224.0000\n",
      "Epoch 293/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 149463672.0000 - mse: 149463664.0000\n",
      "Epoch 294/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 149464630.0000 - mse: 149464624.0000\n",
      "Epoch 295/300\n",
      "40/40 [==============================] - 0s 195us/step - loss: 149412178.0000 - mse: 149412192.0000\n",
      "Epoch 296/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 149323813.0000 - mse: 149323808.0000\n",
      "Epoch 297/300\n",
      "40/40 [==============================] - 0s 268us/step - loss: 149514050.0000 - mse: 149514032.0000\n",
      "Epoch 298/300\n",
      "40/40 [==============================] - 0s 146us/step - loss: 149719331.0000 - mse: 149719328.0000\n",
      "Epoch 299/300\n",
      "40/40 [==============================] - 0s 220us/step - loss: 150476502.0000 - mse: 150476496.0000\n",
      "Epoch 300/300\n",
      "40/40 [==============================] - 0s 171us/step - loss: 149406990.0000 - mse: 149406992.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e8aa3b8f88>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x_train,y_train , batch_size = 10,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[118906.38 ],\n",
       "       [120143.63 ],\n",
       "       [126134.555],\n",
       "       [ 61467.   ],\n",
       "       [171384.45 ],\n",
       "       [121146.055],\n",
       "       [ 51113.098],\n",
       "       [102670.99 ],\n",
       "       [116905.   ],\n",
       "       [157045.08 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103282.38],\n",
       "       [144259.4 ],\n",
       "       [146121.95],\n",
       "       [ 77798.83],\n",
       "       [191050.39],\n",
       "       [105008.31],\n",
       "       [ 81229.06],\n",
       "       [ 97483.56],\n",
       "       [110352.25],\n",
       "       [166187.94]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "accuracy = r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498231027440669"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.save('regressor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save('regressor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[60754.52]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.predict(np.array([[0,1,0,50000,60000,70000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN - Fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Fish.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bream</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Bream</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Bream</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bream</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Bream</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>Smelt</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.0904</td>\n",
       "      <td>1.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>Smelt</td>\n",
       "      <td>13.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>1.2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>Smelt</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.2770</td>\n",
       "      <td>1.2558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>Smelt</td>\n",
       "      <td>19.7</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2.8728</td>\n",
       "      <td>2.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>Smelt</td>\n",
       "      <td>19.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.9322</td>\n",
       "      <td>1.8792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0     Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1     Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2     Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3     Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4     Bream   430.0     26.5     29.0     34.0  12.4440  5.1340\n",
       "..      ...     ...      ...      ...      ...      ...     ...\n",
       "154   Smelt    12.2     11.5     12.2     13.4   2.0904  1.3936\n",
       "155   Smelt    13.4     11.7     12.4     13.5   2.4300  1.2690\n",
       "156   Smelt    12.2     12.1     13.0     13.8   2.2770  1.2558\n",
       "157   Smelt    19.7     13.2     14.3     15.2   2.8728  2.0672\n",
       "158   Smelt    19.9     13.8     15.0     16.2   2.9322  1.8792\n",
       "\n",
       "[159 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bream', 'Roach', 'Whitefish', 'Parkki', 'Perch', 'Pike', 'Smelt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Species    False\n",
       "Weight     False\n",
       "Length1    False\n",
       "Length2    False\n",
       "Length3    False\n",
       "Height     False\n",
       "Width      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['Species'] = le.fit_transform(data['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>2.0904</td>\n",
       "      <td>1.3936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>13.4</td>\n",
       "      <td>11.7</td>\n",
       "      <td>12.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>2.4300</td>\n",
       "      <td>1.2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>5</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2.2770</td>\n",
       "      <td>1.2558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>19.7</td>\n",
       "      <td>13.2</td>\n",
       "      <td>14.3</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2.8728</td>\n",
       "      <td>2.0672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>13.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>2.9322</td>\n",
       "      <td>1.8792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0          0   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1          0   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2          0   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3          0   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4          0   430.0     26.5     29.0     34.0  12.4440  5.1340\n",
       "..       ...     ...      ...      ...      ...      ...     ...\n",
       "154        5    12.2     11.5     12.2     13.4   2.0904  1.3936\n",
       "155        5    13.4     11.7     12.4     13.5   2.4300  1.2690\n",
       "156        5    12.2     12.1     13.0     13.8   2.2770  1.2558\n",
       "157        5    19.7     13.2     14.3     15.2   2.8728  2.0672\n",
       "158        5    19.9     13.8     15.0     16.2   2.9322  1.8792\n",
       "\n",
       "[159 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 6, 1, 2, 3, 5], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,1:7].values\n",
    "y=data.iloc[:,0:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.42000e+02, 2.32000e+01, 2.54000e+01, 3.00000e+01, 1.15200e+01,\n",
       "        4.02000e+00],\n",
       "       [2.90000e+02, 2.40000e+01, 2.63000e+01, 3.12000e+01, 1.24800e+01,\n",
       "        4.30560e+00],\n",
       "       [3.40000e+02, 2.39000e+01, 2.65000e+01, 3.11000e+01, 1.23778e+01,\n",
       "        4.69610e+00],\n",
       "       [3.63000e+02, 2.63000e+01, 2.90000e+01, 3.35000e+01, 1.27300e+01,\n",
       "        4.45550e+00],\n",
       "       [4.30000e+02, 2.65000e+01, 2.90000e+01, 3.40000e+01, 1.24440e+01,\n",
       "        5.13400e+00],\n",
       "       [4.50000e+02, 2.68000e+01, 2.97000e+01, 3.47000e+01, 1.36024e+01,\n",
       "        4.92740e+00],\n",
       "       [5.00000e+02, 2.68000e+01, 2.97000e+01, 3.45000e+01, 1.41795e+01,\n",
       "        5.27850e+00],\n",
       "       [3.90000e+02, 2.76000e+01, 3.00000e+01, 3.50000e+01, 1.26700e+01,\n",
       "        4.69000e+00],\n",
       "       [4.50000e+02, 2.76000e+01, 3.00000e+01, 3.51000e+01, 1.40049e+01,\n",
       "        4.84380e+00],\n",
       "       [5.00000e+02, 2.85000e+01, 3.07000e+01, 3.62000e+01, 1.42266e+01,\n",
       "        4.95940e+00],\n",
       "       [4.75000e+02, 2.84000e+01, 3.10000e+01, 3.62000e+01, 1.42628e+01,\n",
       "        5.10420e+00],\n",
       "       [5.00000e+02, 2.87000e+01, 3.10000e+01, 3.62000e+01, 1.43714e+01,\n",
       "        4.81460e+00],\n",
       "       [5.00000e+02, 2.91000e+01, 3.15000e+01, 3.64000e+01, 1.37592e+01,\n",
       "        4.36800e+00],\n",
       "       [3.40000e+02, 2.95000e+01, 3.20000e+01, 3.73000e+01, 1.39129e+01,\n",
       "        5.07280e+00],\n",
       "       [6.00000e+02, 2.94000e+01, 3.20000e+01, 3.72000e+01, 1.49544e+01,\n",
       "        5.17080e+00],\n",
       "       [6.00000e+02, 2.94000e+01, 3.20000e+01, 3.72000e+01, 1.54380e+01,\n",
       "        5.58000e+00],\n",
       "       [7.00000e+02, 3.04000e+01, 3.30000e+01, 3.83000e+01, 1.48604e+01,\n",
       "        5.28540e+00],\n",
       "       [7.00000e+02, 3.04000e+01, 3.30000e+01, 3.85000e+01, 1.49380e+01,\n",
       "        5.19750e+00],\n",
       "       [6.10000e+02, 3.09000e+01, 3.35000e+01, 3.86000e+01, 1.56330e+01,\n",
       "        5.13380e+00],\n",
       "       [6.50000e+02, 3.10000e+01, 3.35000e+01, 3.87000e+01, 1.44738e+01,\n",
       "        5.72760e+00],\n",
       "       [5.75000e+02, 3.13000e+01, 3.40000e+01, 3.95000e+01, 1.51285e+01,\n",
       "        5.56950e+00],\n",
       "       [6.85000e+02, 3.14000e+01, 3.40000e+01, 3.92000e+01, 1.59936e+01,\n",
       "        5.37040e+00],\n",
       "       [6.20000e+02, 3.15000e+01, 3.45000e+01, 3.97000e+01, 1.55227e+01,\n",
       "        5.28010e+00],\n",
       "       [6.80000e+02, 3.18000e+01, 3.50000e+01, 4.06000e+01, 1.54686e+01,\n",
       "        6.13060e+00],\n",
       "       [7.00000e+02, 3.19000e+01, 3.50000e+01, 4.05000e+01, 1.62405e+01,\n",
       "        5.58900e+00],\n",
       "       [7.25000e+02, 3.18000e+01, 3.50000e+01, 4.09000e+01, 1.63600e+01,\n",
       "        6.05320e+00],\n",
       "       [7.20000e+02, 3.20000e+01, 3.50000e+01, 4.06000e+01, 1.63618e+01,\n",
       "        6.09000e+00],\n",
       "       [7.14000e+02, 3.27000e+01, 3.60000e+01, 4.15000e+01, 1.65170e+01,\n",
       "        5.85150e+00],\n",
       "       [8.50000e+02, 3.28000e+01, 3.60000e+01, 4.16000e+01, 1.68896e+01,\n",
       "        6.19840e+00],\n",
       "       [1.00000e+03, 3.35000e+01, 3.70000e+01, 4.26000e+01, 1.89570e+01,\n",
       "        6.60300e+00],\n",
       "       [9.20000e+02, 3.50000e+01, 3.85000e+01, 4.41000e+01, 1.80369e+01,\n",
       "        6.30630e+00],\n",
       "       [9.55000e+02, 3.50000e+01, 3.85000e+01, 4.40000e+01, 1.80840e+01,\n",
       "        6.29200e+00],\n",
       "       [9.25000e+02, 3.62000e+01, 3.95000e+01, 4.53000e+01, 1.87542e+01,\n",
       "        6.74970e+00],\n",
       "       [9.75000e+02, 3.74000e+01, 4.10000e+01, 4.59000e+01, 1.86354e+01,\n",
       "        6.74730e+00],\n",
       "       [9.50000e+02, 3.80000e+01, 4.10000e+01, 4.65000e+01, 1.76235e+01,\n",
       "        6.37050e+00],\n",
       "       [4.00000e+01, 1.29000e+01, 1.41000e+01, 1.62000e+01, 4.14720e+00,\n",
       "        2.26800e+00],\n",
       "       [6.90000e+01, 1.65000e+01, 1.82000e+01, 2.03000e+01, 5.29830e+00,\n",
       "        2.82170e+00],\n",
       "       [7.80000e+01, 1.75000e+01, 1.88000e+01, 2.12000e+01, 5.57560e+00,\n",
       "        2.90440e+00],\n",
       "       [8.70000e+01, 1.82000e+01, 1.98000e+01, 2.22000e+01, 5.61660e+00,\n",
       "        3.17460e+00],\n",
       "       [1.20000e+02, 1.86000e+01, 2.00000e+01, 2.22000e+01, 6.21600e+00,\n",
       "        3.57420e+00],\n",
       "       [0.00000e+00, 1.90000e+01, 2.05000e+01, 2.28000e+01, 6.47520e+00,\n",
       "        3.35160e+00],\n",
       "       [1.10000e+02, 1.91000e+01, 2.08000e+01, 2.31000e+01, 6.16770e+00,\n",
       "        3.39570e+00],\n",
       "       [1.20000e+02, 1.94000e+01, 2.10000e+01, 2.37000e+01, 6.11460e+00,\n",
       "        3.29430e+00],\n",
       "       [1.50000e+02, 2.04000e+01, 2.20000e+01, 2.47000e+01, 5.80450e+00,\n",
       "        3.75440e+00],\n",
       "       [1.45000e+02, 2.05000e+01, 2.20000e+01, 2.43000e+01, 6.63390e+00,\n",
       "        3.54780e+00],\n",
       "       [1.60000e+02, 2.05000e+01, 2.25000e+01, 2.53000e+01, 7.03340e+00,\n",
       "        3.82030e+00],\n",
       "       [1.40000e+02, 2.10000e+01, 2.25000e+01, 2.50000e+01, 6.55000e+00,\n",
       "        3.32500e+00],\n",
       "       [1.60000e+02, 2.11000e+01, 2.25000e+01, 2.50000e+01, 6.40000e+00,\n",
       "        3.80000e+00],\n",
       "       [1.69000e+02, 2.20000e+01, 2.40000e+01, 2.72000e+01, 7.53440e+00,\n",
       "        3.83520e+00],\n",
       "       [1.61000e+02, 2.20000e+01, 2.34000e+01, 2.67000e+01, 6.91530e+00,\n",
       "        3.63120e+00],\n",
       "       [2.00000e+02, 2.21000e+01, 2.35000e+01, 2.68000e+01, 7.39680e+00,\n",
       "        4.12720e+00],\n",
       "       [1.80000e+02, 2.36000e+01, 2.52000e+01, 2.79000e+01, 7.08660e+00,\n",
       "        3.90600e+00],\n",
       "       [2.90000e+02, 2.40000e+01, 2.60000e+01, 2.92000e+01, 8.87680e+00,\n",
       "        4.49680e+00],\n",
       "       [2.72000e+02, 2.50000e+01, 2.70000e+01, 3.06000e+01, 8.56800e+00,\n",
       "        4.77360e+00],\n",
       "       [3.90000e+02, 2.95000e+01, 3.17000e+01, 3.50000e+01, 9.48500e+00,\n",
       "        5.35500e+00],\n",
       "       [2.70000e+02, 2.36000e+01, 2.60000e+01, 2.87000e+01, 8.38040e+00,\n",
       "        4.24760e+00],\n",
       "       [2.70000e+02, 2.41000e+01, 2.65000e+01, 2.93000e+01, 8.14540e+00,\n",
       "        4.24850e+00],\n",
       "       [3.06000e+02, 2.56000e+01, 2.80000e+01, 3.08000e+01, 8.77800e+00,\n",
       "        4.68160e+00],\n",
       "       [5.40000e+02, 2.85000e+01, 3.10000e+01, 3.40000e+01, 1.07440e+01,\n",
       "        6.56200e+00],\n",
       "       [8.00000e+02, 3.37000e+01, 3.64000e+01, 3.96000e+01, 1.17612e+01,\n",
       "        6.57360e+00],\n",
       "       [1.00000e+03, 3.73000e+01, 4.00000e+01, 4.35000e+01, 1.23540e+01,\n",
       "        6.52500e+00],\n",
       "       [5.50000e+01, 1.35000e+01, 1.47000e+01, 1.65000e+01, 6.84750e+00,\n",
       "        2.32650e+00],\n",
       "       [6.00000e+01, 1.43000e+01, 1.55000e+01, 1.74000e+01, 6.57720e+00,\n",
       "        2.31420e+00],\n",
       "       [9.00000e+01, 1.63000e+01, 1.77000e+01, 1.98000e+01, 7.40520e+00,\n",
       "        2.67300e+00],\n",
       "       [1.20000e+02, 1.75000e+01, 1.90000e+01, 2.13000e+01, 8.39220e+00,\n",
       "        2.91810e+00],\n",
       "       [1.50000e+02, 1.84000e+01, 2.00000e+01, 2.24000e+01, 8.89280e+00,\n",
       "        3.29280e+00],\n",
       "       [1.40000e+02, 1.90000e+01, 2.07000e+01, 2.32000e+01, 8.53760e+00,\n",
       "        3.29440e+00],\n",
       "       [1.70000e+02, 1.90000e+01, 2.07000e+01, 2.32000e+01, 9.39600e+00,\n",
       "        3.41040e+00],\n",
       "       [1.45000e+02, 1.98000e+01, 2.15000e+01, 2.41000e+01, 9.73640e+00,\n",
       "        3.15710e+00],\n",
       "       [2.00000e+02, 2.12000e+01, 2.30000e+01, 2.58000e+01, 1.03458e+01,\n",
       "        3.66360e+00],\n",
       "       [2.73000e+02, 2.30000e+01, 2.50000e+01, 2.80000e+01, 1.10880e+01,\n",
       "        4.14400e+00],\n",
       "       [3.00000e+02, 2.40000e+01, 2.60000e+01, 2.90000e+01, 1.13680e+01,\n",
       "        4.23400e+00],\n",
       "       [5.90000e+00, 7.50000e+00, 8.40000e+00, 8.80000e+00, 2.11200e+00,\n",
       "        1.40800e+00],\n",
       "       [3.20000e+01, 1.25000e+01, 1.37000e+01, 1.47000e+01, 3.52800e+00,\n",
       "        1.99920e+00],\n",
       "       [4.00000e+01, 1.38000e+01, 1.50000e+01, 1.60000e+01, 3.82400e+00,\n",
       "        2.43200e+00],\n",
       "       [5.15000e+01, 1.50000e+01, 1.62000e+01, 1.72000e+01, 4.59240e+00,\n",
       "        2.63160e+00],\n",
       "       [7.00000e+01, 1.57000e+01, 1.74000e+01, 1.85000e+01, 4.58800e+00,\n",
       "        2.94150e+00],\n",
       "       [1.00000e+02, 1.62000e+01, 1.80000e+01, 1.92000e+01, 5.22240e+00,\n",
       "        3.32160e+00],\n",
       "       [7.80000e+01, 1.68000e+01, 1.87000e+01, 1.94000e+01, 5.19920e+00,\n",
       "        3.12340e+00],\n",
       "       [8.00000e+01, 1.72000e+01, 1.90000e+01, 2.02000e+01, 5.63580e+00,\n",
       "        3.05020e+00],\n",
       "       [8.50000e+01, 1.78000e+01, 1.96000e+01, 2.08000e+01, 5.13760e+00,\n",
       "        3.03680e+00],\n",
       "       [8.50000e+01, 1.82000e+01, 2.00000e+01, 2.10000e+01, 5.08200e+00,\n",
       "        2.77200e+00],\n",
       "       [1.10000e+02, 1.90000e+01, 2.10000e+01, 2.25000e+01, 5.69250e+00,\n",
       "        3.55500e+00],\n",
       "       [1.15000e+02, 1.90000e+01, 2.10000e+01, 2.25000e+01, 5.91750e+00,\n",
       "        3.30750e+00],\n",
       "       [1.25000e+02, 1.90000e+01, 2.10000e+01, 2.25000e+01, 5.69250e+00,\n",
       "        3.66750e+00],\n",
       "       [1.30000e+02, 1.93000e+01, 2.13000e+01, 2.28000e+01, 6.38400e+00,\n",
       "        3.53400e+00],\n",
       "       [1.20000e+02, 2.00000e+01, 2.20000e+01, 2.35000e+01, 6.11000e+00,\n",
       "        3.40750e+00],\n",
       "       [1.20000e+02, 2.00000e+01, 2.20000e+01, 2.35000e+01, 5.64000e+00,\n",
       "        3.52500e+00],\n",
       "       [1.30000e+02, 2.00000e+01, 2.20000e+01, 2.35000e+01, 6.11000e+00,\n",
       "        3.52500e+00],\n",
       "       [1.35000e+02, 2.00000e+01, 2.20000e+01, 2.35000e+01, 5.87500e+00,\n",
       "        3.52500e+00],\n",
       "       [1.10000e+02, 2.00000e+01, 2.20000e+01, 2.35000e+01, 5.52250e+00,\n",
       "        3.99500e+00],\n",
       "       [1.30000e+02, 2.05000e+01, 2.25000e+01, 2.40000e+01, 5.85600e+00,\n",
       "        3.62400e+00],\n",
       "       [1.50000e+02, 2.05000e+01, 2.25000e+01, 2.40000e+01, 6.79200e+00,\n",
       "        3.62400e+00],\n",
       "       [1.45000e+02, 2.07000e+01, 2.27000e+01, 2.42000e+01, 5.95320e+00,\n",
       "        3.63000e+00],\n",
       "       [1.50000e+02, 2.10000e+01, 2.30000e+01, 2.45000e+01, 5.21850e+00,\n",
       "        3.62600e+00],\n",
       "       [1.70000e+02, 2.15000e+01, 2.35000e+01, 2.50000e+01, 6.27500e+00,\n",
       "        3.72500e+00],\n",
       "       [2.25000e+02, 2.20000e+01, 2.40000e+01, 2.55000e+01, 7.29300e+00,\n",
       "        3.72300e+00],\n",
       "       [1.45000e+02, 2.20000e+01, 2.40000e+01, 2.55000e+01, 6.37500e+00,\n",
       "        3.82500e+00],\n",
       "       [1.88000e+02, 2.26000e+01, 2.46000e+01, 2.62000e+01, 6.73340e+00,\n",
       "        4.16580e+00],\n",
       "       [1.80000e+02, 2.30000e+01, 2.50000e+01, 2.65000e+01, 6.43950e+00,\n",
       "        3.68350e+00],\n",
       "       [1.97000e+02, 2.35000e+01, 2.56000e+01, 2.70000e+01, 6.56100e+00,\n",
       "        4.23900e+00],\n",
       "       [2.18000e+02, 2.50000e+01, 2.65000e+01, 2.80000e+01, 7.16800e+00,\n",
       "        4.14400e+00],\n",
       "       [3.00000e+02, 2.52000e+01, 2.73000e+01, 2.87000e+01, 8.32300e+00,\n",
       "        5.13730e+00],\n",
       "       [2.60000e+02, 2.54000e+01, 2.75000e+01, 2.89000e+01, 7.16720e+00,\n",
       "        4.33500e+00],\n",
       "       [2.65000e+02, 2.54000e+01, 2.75000e+01, 2.89000e+01, 7.05160e+00,\n",
       "        4.33500e+00],\n",
       "       [2.50000e+02, 2.54000e+01, 2.75000e+01, 2.89000e+01, 7.28280e+00,\n",
       "        4.56620e+00],\n",
       "       [2.50000e+02, 2.59000e+01, 2.80000e+01, 2.94000e+01, 7.82040e+00,\n",
       "        4.20420e+00],\n",
       "       [3.00000e+02, 2.69000e+01, 2.87000e+01, 3.01000e+01, 7.58520e+00,\n",
       "        4.63540e+00],\n",
       "       [3.20000e+02, 2.78000e+01, 3.00000e+01, 3.16000e+01, 7.61560e+00,\n",
       "        4.77160e+00],\n",
       "       [5.14000e+02, 3.05000e+01, 3.28000e+01, 3.40000e+01, 1.00300e+01,\n",
       "        6.01800e+00],\n",
       "       [5.56000e+02, 3.20000e+01, 3.45000e+01, 3.65000e+01, 1.02565e+01,\n",
       "        6.38750e+00],\n",
       "       [8.40000e+02, 3.25000e+01, 3.50000e+01, 3.73000e+01, 1.14884e+01,\n",
       "        7.79570e+00],\n",
       "       [6.85000e+02, 3.40000e+01, 3.65000e+01, 3.90000e+01, 1.08810e+01,\n",
       "        6.86400e+00],\n",
       "       [7.00000e+02, 3.40000e+01, 3.60000e+01, 3.83000e+01, 1.06091e+01,\n",
       "        6.74080e+00],\n",
       "       [7.00000e+02, 3.45000e+01, 3.70000e+01, 3.94000e+01, 1.08350e+01,\n",
       "        6.26460e+00],\n",
       "       [6.90000e+02, 3.46000e+01, 3.70000e+01, 3.93000e+01, 1.05717e+01,\n",
       "        6.36660e+00],\n",
       "       [9.00000e+02, 3.65000e+01, 3.90000e+01, 4.14000e+01, 1.11366e+01,\n",
       "        7.49340e+00],\n",
       "       [6.50000e+02, 3.65000e+01, 3.90000e+01, 4.14000e+01, 1.11366e+01,\n",
       "        6.00300e+00],\n",
       "       [8.20000e+02, 3.66000e+01, 3.90000e+01, 4.13000e+01, 1.24313e+01,\n",
       "        7.35140e+00],\n",
       "       [8.50000e+02, 3.69000e+01, 4.00000e+01, 4.23000e+01, 1.19286e+01,\n",
       "        7.10640e+00],\n",
       "       [9.00000e+02, 3.70000e+01, 4.00000e+01, 4.25000e+01, 1.17300e+01,\n",
       "        7.22500e+00],\n",
       "       [1.01500e+03, 3.70000e+01, 4.00000e+01, 4.24000e+01, 1.23808e+01,\n",
       "        7.46240e+00],\n",
       "       [8.20000e+02, 3.71000e+01, 4.00000e+01, 4.25000e+01, 1.11350e+01,\n",
       "        6.63000e+00],\n",
       "       [1.10000e+03, 3.90000e+01, 4.20000e+01, 4.46000e+01, 1.28002e+01,\n",
       "        6.86840e+00],\n",
       "       [1.00000e+03, 3.98000e+01, 4.30000e+01, 4.52000e+01, 1.19328e+01,\n",
       "        7.27720e+00],\n",
       "       [1.10000e+03, 4.01000e+01, 4.30000e+01, 4.55000e+01, 1.25125e+01,\n",
       "        7.41650e+00],\n",
       "       [1.00000e+03, 4.02000e+01, 4.35000e+01, 4.60000e+01, 1.26040e+01,\n",
       "        8.14200e+00],\n",
       "       [1.00000e+03, 4.11000e+01, 4.40000e+01, 4.66000e+01, 1.24888e+01,\n",
       "        7.59580e+00],\n",
       "       [2.00000e+02, 3.00000e+01, 3.23000e+01, 3.48000e+01, 5.56800e+00,\n",
       "        3.37560e+00],\n",
       "       [3.00000e+02, 3.17000e+01, 3.40000e+01, 3.78000e+01, 5.70780e+00,\n",
       "        4.15800e+00],\n",
       "       [3.00000e+02, 3.27000e+01, 3.50000e+01, 3.88000e+01, 5.93640e+00,\n",
       "        4.38440e+00],\n",
       "       [3.00000e+02, 3.48000e+01, 3.73000e+01, 3.98000e+01, 6.28840e+00,\n",
       "        4.01980e+00],\n",
       "       [4.30000e+02, 3.55000e+01, 3.80000e+01, 4.05000e+01, 7.29000e+00,\n",
       "        4.57650e+00],\n",
       "       [3.45000e+02, 3.60000e+01, 3.85000e+01, 4.10000e+01, 6.39600e+00,\n",
       "        3.97700e+00],\n",
       "       [4.56000e+02, 4.00000e+01, 4.25000e+01, 4.55000e+01, 7.28000e+00,\n",
       "        4.32250e+00],\n",
       "       [5.10000e+02, 4.00000e+01, 4.25000e+01, 4.55000e+01, 6.82500e+00,\n",
       "        4.45900e+00],\n",
       "       [5.40000e+02, 4.01000e+01, 4.30000e+01, 4.58000e+01, 7.78600e+00,\n",
       "        5.12960e+00],\n",
       "       [5.00000e+02, 4.20000e+01, 4.50000e+01, 4.80000e+01, 6.96000e+00,\n",
       "        4.89600e+00],\n",
       "       [5.67000e+02, 4.32000e+01, 4.60000e+01, 4.87000e+01, 7.79200e+00,\n",
       "        4.87000e+00],\n",
       "       [7.70000e+02, 4.48000e+01, 4.80000e+01, 5.12000e+01, 7.68000e+00,\n",
       "        5.37600e+00],\n",
       "       [9.50000e+02, 4.83000e+01, 5.17000e+01, 5.51000e+01, 8.92620e+00,\n",
       "        6.17120e+00],\n",
       "       [1.25000e+03, 5.20000e+01, 5.60000e+01, 5.97000e+01, 1.06863e+01,\n",
       "        6.98490e+00],\n",
       "       [1.60000e+03, 5.60000e+01, 6.00000e+01, 6.40000e+01, 9.60000e+00,\n",
       "        6.14400e+00],\n",
       "       [1.55000e+03, 5.60000e+01, 6.00000e+01, 6.40000e+01, 9.60000e+00,\n",
       "        6.14400e+00],\n",
       "       [1.65000e+03, 5.90000e+01, 6.34000e+01, 6.80000e+01, 1.08120e+01,\n",
       "        7.48000e+00],\n",
       "       [6.70000e+00, 9.30000e+00, 9.80000e+00, 1.08000e+01, 1.73880e+00,\n",
       "        1.04760e+00],\n",
       "       [7.50000e+00, 1.00000e+01, 1.05000e+01, 1.16000e+01, 1.97200e+00,\n",
       "        1.16000e+00],\n",
       "       [7.00000e+00, 1.01000e+01, 1.06000e+01, 1.16000e+01, 1.72840e+00,\n",
       "        1.14840e+00],\n",
       "       [9.70000e+00, 1.04000e+01, 1.10000e+01, 1.20000e+01, 2.19600e+00,\n",
       "        1.38000e+00],\n",
       "       [9.80000e+00, 1.07000e+01, 1.12000e+01, 1.24000e+01, 2.08320e+00,\n",
       "        1.27720e+00],\n",
       "       [8.70000e+00, 1.08000e+01, 1.13000e+01, 1.26000e+01, 1.97820e+00,\n",
       "        1.28520e+00],\n",
       "       [1.00000e+01, 1.13000e+01, 1.18000e+01, 1.31000e+01, 2.21390e+00,\n",
       "        1.28380e+00],\n",
       "       [9.90000e+00, 1.13000e+01, 1.18000e+01, 1.31000e+01, 2.21390e+00,\n",
       "        1.16590e+00],\n",
       "       [9.80000e+00, 1.14000e+01, 1.20000e+01, 1.32000e+01, 2.20440e+00,\n",
       "        1.14840e+00],\n",
       "       [1.22000e+01, 1.15000e+01, 1.22000e+01, 1.34000e+01, 2.09040e+00,\n",
       "        1.39360e+00],\n",
       "       [1.34000e+01, 1.17000e+01, 1.24000e+01, 1.35000e+01, 2.43000e+00,\n",
       "        1.26900e+00],\n",
       "       [1.22000e+01, 1.21000e+01, 1.30000e+01, 1.38000e+01, 2.27700e+00,\n",
       "        1.25580e+00],\n",
       "       [1.97000e+01, 1.32000e+01, 1.43000e+01, 1.52000e+01, 2.87280e+00,\n",
       "        2.06720e+00],\n",
       "       [1.99000e+01, 1.38000e+01, 1.50000e+01, 1.62000e+01, 2.93220e+00,\n",
       "        1.87920e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one = OneHotEncoder()\n",
    "z=one.fit_transform(y[:,0:1]).toarray()\n",
    "y=np.delete(y,0,axis=1)\n",
    "y=np.concatenate((z,y),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 7)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=6, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 6,init = 'random_uniform',activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=9, activation=\"relu\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units =9,init = 'random_uniform',activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=7, activation=\"softmax\", kernel_initializer=\"random_uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units = 7 ,init = 'random_uniform',activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile (optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 1.9424 - accuracy: 0.3780\n",
      "Epoch 2/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.9348 - accuracy: 0.3701\n",
      "Epoch 3/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.9255 - accuracy: 0.3701\n",
      "Epoch 4/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.9126 - accuracy: 0.3701\n",
      "Epoch 5/300\n",
      "127/127 [==============================] - 0s 144us/step - loss: 1.8938 - accuracy: 0.3701\n",
      "Epoch 6/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.8666 - accuracy: 0.3701\n",
      "Epoch 7/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.8267 - accuracy: 0.3701\n",
      "Epoch 8/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.7755 - accuracy: 0.3701\n",
      "Epoch 9/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.7137 - accuracy: 0.3701\n",
      "Epoch 10/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.6534 - accuracy: 0.3701\n",
      "Epoch 11/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.5908 - accuracy: 0.3701\n",
      "Epoch 12/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.5407 - accuracy: 0.3701\n",
      "Epoch 13/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.4979 - accuracy: 0.3701\n",
      "Epoch 14/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.4609 - accuracy: 0.3701\n",
      "Epoch 15/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.4311 - accuracy: 0.3701\n",
      "Epoch 16/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.4035 - accuracy: 0.4016\n",
      "Epoch 17/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.3810 - accuracy: 0.4567\n",
      "Epoch 18/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.3618 - accuracy: 0.4567\n",
      "Epoch 19/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.3466 - accuracy: 0.4646\n",
      "Epoch 20/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.3310 - accuracy: 0.4646\n",
      "Epoch 21/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.3207 - accuracy: 0.4646\n",
      "Epoch 22/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.3086 - accuracy: 0.4646\n",
      "Epoch 23/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.3015 - accuracy: 0.4646\n",
      "Epoch 24/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.2929 - accuracy: 0.4646\n",
      "Epoch 25/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2851 - accuracy: 0.4646\n",
      "Epoch 26/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.2787 - accuracy: 0.4724\n",
      "Epoch 27/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2715 - accuracy: 0.4803\n",
      "Epoch 28/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.2678 - accuracy: 0.5433\n",
      "Epoch 29/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2613 - accuracy: 0.5512\n",
      "Epoch 30/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.2589 - accuracy: 0.5512\n",
      "Epoch 31/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.2519 - accuracy: 0.5512\n",
      "Epoch 32/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.2477 - accuracy: 0.5512\n",
      "Epoch 33/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.2443 - accuracy: 0.5512\n",
      "Epoch 34/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2421 - accuracy: 0.5512\n",
      "Epoch 35/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.2386 - accuracy: 0.5433\n",
      "Epoch 36/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2340 - accuracy: 0.5433\n",
      "Epoch 37/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2314 - accuracy: 0.5591\n",
      "Epoch 38/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2274 - accuracy: 0.5512\n",
      "Epoch 39/300\n",
      "127/127 [==============================] - 0s 231us/step - loss: 1.2256 - accuracy: 0.5433\n",
      "Epoch 40/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.2233 - accuracy: 0.5512\n",
      "Epoch 41/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.2199 - accuracy: 0.5591\n",
      "Epoch 42/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2173 - accuracy: 0.5591\n",
      "Epoch 43/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.2145 - accuracy: 0.5591\n",
      "Epoch 44/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2133 - accuracy: 0.5512\n",
      "Epoch 45/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.2098 - accuracy: 0.5512\n",
      "Epoch 46/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.2076 - accuracy: 0.5591\n",
      "Epoch 47/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.2068 - accuracy: 0.5591\n",
      "Epoch 48/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.2037 - accuracy: 0.5669\n",
      "Epoch 49/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.2011 - accuracy: 0.5512\n",
      "Epoch 50/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.1988 - accuracy: 0.5591\n",
      "Epoch 51/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.1966 - accuracy: 0.5512\n",
      "Epoch 52/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.1959 - accuracy: 0.5512\n",
      "Epoch 53/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1947 - accuracy: 0.5512\n",
      "Epoch 54/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.1913 - accuracy: 0.5591\n",
      "Epoch 55/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.1893 - accuracy: 0.5512\n",
      "Epoch 56/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.1866 - accuracy: 0.5512\n",
      "Epoch 57/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.1859 - accuracy: 0.5591\n",
      "Epoch 58/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1827 - accuracy: 0.5512\n",
      "Epoch 59/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1802 - accuracy: 0.5591\n",
      "Epoch 60/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1785 - accuracy: 0.5591\n",
      "Epoch 61/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1773 - accuracy: 0.5512\n",
      "Epoch 62/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1764 - accuracy: 0.5512\n",
      "Epoch 63/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1735 - accuracy: 0.5512\n",
      "Epoch 64/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1713 - accuracy: 0.5512\n",
      "Epoch 65/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1691 - accuracy: 0.5512\n",
      "Epoch 66/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1648 - accuracy: 0.5512\n",
      "Epoch 67/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1605 - accuracy: 0.5512\n",
      "Epoch 68/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1578 - accuracy: 0.5591\n",
      "Epoch 69/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1566 - accuracy: 0.5591\n",
      "Epoch 70/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1559 - accuracy: 0.5512\n",
      "Epoch 71/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1512 - accuracy: 0.5512\n",
      "Epoch 72/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1514 - accuracy: 0.5512\n",
      "Epoch 73/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.1481 - accuracy: 0.5512\n",
      "Epoch 74/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1456 - accuracy: 0.5512\n",
      "Epoch 75/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1440 - accuracy: 0.5512\n",
      "Epoch 76/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1425 - accuracy: 0.5512\n",
      "Epoch 77/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1402 - accuracy: 0.5512\n",
      "Epoch 78/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1402 - accuracy: 0.5512\n",
      "Epoch 79/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1396 - accuracy: 0.5512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1356 - accuracy: 0.5512\n",
      "Epoch 81/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1331 - accuracy: 0.5591\n",
      "Epoch 82/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1327 - accuracy: 0.5512\n",
      "Epoch 83/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1314 - accuracy: 0.5512\n",
      "Epoch 84/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1291 - accuracy: 0.5512\n",
      "Epoch 85/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.1270 - accuracy: 0.5512\n",
      "Epoch 86/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1251 - accuracy: 0.5512\n",
      "Epoch 87/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1224 - accuracy: 0.5512\n",
      "Epoch 88/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1202 - accuracy: 0.5512\n",
      "Epoch 89/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.1204 - accuracy: 0.5512\n",
      "Epoch 90/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.1173 - accuracy: 0.5512\n",
      "Epoch 91/300\n",
      "127/127 [==============================] - 0s 184us/step - loss: 1.1187 - accuracy: 0.5512\n",
      "Epoch 92/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.1130 - accuracy: 0.5512\n",
      "Epoch 93/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.1135 - accuracy: 0.5512\n",
      "Epoch 94/300\n",
      "127/127 [==============================] - 0s 177us/step - loss: 1.1115 - accuracy: 0.5591\n",
      "Epoch 95/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.1112 - accuracy: 0.5591\n",
      "Epoch 96/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.1079 - accuracy: 0.5512\n",
      "Epoch 97/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.1064 - accuracy: 0.5512\n",
      "Epoch 98/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.1043 - accuracy: 0.5512\n",
      "Epoch 99/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.1059 - accuracy: 0.5591\n",
      "Epoch 100/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.1030 - accuracy: 0.5512\n",
      "Epoch 101/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.1009 - accuracy: 0.5512\n",
      "Epoch 102/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.0992 - accuracy: 0.5591\n",
      "Epoch 103/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.1005 - accuracy: 0.5669\n",
      "Epoch 104/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0980 - accuracy: 0.5591\n",
      "Epoch 105/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0959 - accuracy: 0.5591\n",
      "Epoch 106/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0934 - accuracy: 0.5591\n",
      "Epoch 107/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.0942 - accuracy: 0.5591\n",
      "Epoch 108/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.0905 - accuracy: 0.5591\n",
      "Epoch 109/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.0891 - accuracy: 0.5591\n",
      "Epoch 110/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0888 - accuracy: 0.5591\n",
      "Epoch 111/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0881 - accuracy: 0.5591\n",
      "Epoch 112/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0867 - accuracy: 0.5591\n",
      "Epoch 113/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0838 - accuracy: 0.5591\n",
      "Epoch 114/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0819 - accuracy: 0.5591\n",
      "Epoch 115/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.0808 - accuracy: 0.5591\n",
      "Epoch 116/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0807 - accuracy: 0.5669\n",
      "Epoch 117/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0783 - accuracy: 0.5669\n",
      "Epoch 118/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0773 - accuracy: 0.5591\n",
      "Epoch 119/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 1.0772 - accuracy: 0.5669\n",
      "Epoch 120/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0763 - accuracy: 0.5591\n",
      "Epoch 121/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0738 - accuracy: 0.5591\n",
      "Epoch 122/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0729 - accuracy: 0.5669\n",
      "Epoch 123/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0715 - accuracy: 0.5669\n",
      "Epoch 124/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0701 - accuracy: 0.5591\n",
      "Epoch 125/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0729 - accuracy: 0.5591\n",
      "Epoch 126/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0674 - accuracy: 0.5669\n",
      "Epoch 127/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.0679 - accuracy: 0.5669\n",
      "Epoch 128/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.0660 - accuracy: 0.5669\n",
      "Epoch 129/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0653 - accuracy: 0.5669\n",
      "Epoch 130/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0642 - accuracy: 0.5591\n",
      "Epoch 131/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0624 - accuracy: 0.5591\n",
      "Epoch 132/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0623 - accuracy: 0.5669\n",
      "Epoch 133/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0594 - accuracy: 0.5591\n",
      "Epoch 134/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.0613 - accuracy: 0.5669\n",
      "Epoch 135/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0579 - accuracy: 0.5669\n",
      "Epoch 136/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0569 - accuracy: 0.5591\n",
      "Epoch 137/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0571 - accuracy: 0.5591\n",
      "Epoch 138/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0568 - accuracy: 0.5669\n",
      "Epoch 139/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.0550 - accuracy: 0.5591\n",
      "Epoch 140/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0526 - accuracy: 0.5669\n",
      "Epoch 141/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0526 - accuracy: 0.5591\n",
      "Epoch 142/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0504 - accuracy: 0.5591\n",
      "Epoch 143/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0490 - accuracy: 0.5669\n",
      "Epoch 144/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0480 - accuracy: 0.5669\n",
      "Epoch 145/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0477 - accuracy: 0.5669\n",
      "Epoch 146/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.0476 - accuracy: 0.5591\n",
      "Epoch 147/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0440 - accuracy: 0.5669\n",
      "Epoch 148/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0462 - accuracy: 0.5591\n",
      "Epoch 149/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0421 - accuracy: 0.5669\n",
      "Epoch 150/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0424 - accuracy: 0.5669\n",
      "Epoch 151/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0421 - accuracy: 0.5669\n",
      "Epoch 152/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 1.0406 - accuracy: 0.5591\n",
      "Epoch 153/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0392 - accuracy: 0.5591\n",
      "Epoch 154/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0384 - accuracy: 0.5669\n",
      "Epoch 155/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 1.0383 - accuracy: 0.5591\n",
      "Epoch 156/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 1.0362 - accuracy: 0.5669\n",
      "Epoch 157/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 1.0340 - accuracy: 0.5669\n",
      "Epoch 158/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.0342 - accuracy: 0.5669\n",
      "Epoch 159/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 1.0326 - accuracy: 0.5669\n",
      "Epoch 160/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0314 - accuracy: 0.5669\n",
      "Epoch 161/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0311 - accuracy: 0.5669\n",
      "Epoch 162/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0279 - accuracy: 0.5669\n",
      "Epoch 163/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0267 - accuracy: 0.5669\n",
      "Epoch 164/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0283 - accuracy: 0.5591\n",
      "Epoch 165/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0302 - accuracy: 0.5591\n",
      "Epoch 166/300\n",
      "127/127 [==============================] - 0s 177us/step - loss: 1.0240 - accuracy: 0.5669\n",
      "Epoch 167/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0222 - accuracy: 0.5591\n",
      "Epoch 168/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0213 - accuracy: 0.5669\n",
      "Epoch 169/300\n",
      "127/127 [==============================] - ETA: 0s - loss: 1.0541 - accuracy: 0.60 - 0s 146us/step - loss: 1.0181 - accuracy: 0.5669\n",
      "Epoch 170/300\n",
      "127/127 [==============================] - 0s 169us/step - loss: 1.0175 - accuracy: 0.5669\n",
      "Epoch 171/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0159 - accuracy: 0.5669\n",
      "Epoch 172/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 1.0152 - accuracy: 0.5669\n",
      "Epoch 173/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0129 - accuracy: 0.5669\n",
      "Epoch 174/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0113 - accuracy: 0.5669\n",
      "Epoch 175/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0106 - accuracy: 0.5669\n",
      "Epoch 176/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 1.0079 - accuracy: 0.5669\n",
      "Epoch 177/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0084 - accuracy: 0.5669\n",
      "Epoch 178/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0051 - accuracy: 0.5669\n",
      "Epoch 179/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0047 - accuracy: 0.5669\n",
      "Epoch 180/300\n",
      "127/127 [==============================] - 0s 138us/step - loss: 1.0034 - accuracy: 0.5669\n",
      "Epoch 181/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.9996 - accuracy: 0.5669\n",
      "Epoch 182/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.9990 - accuracy: 0.5669\n",
      "Epoch 183/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.9950 - accuracy: 0.5669\n",
      "Epoch 184/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.9929 - accuracy: 0.5669\n",
      "Epoch 185/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9968 - accuracy: 0.5591\n",
      "Epoch 186/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9855 - accuracy: 0.5669\n",
      "Epoch 187/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9859 - accuracy: 0.5591\n",
      "Epoch 188/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.9820 - accuracy: 0.5669\n",
      "Epoch 189/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.9815 - accuracy: 0.5669\n",
      "Epoch 190/300\n",
      "127/127 [==============================] - ETA: 0s - loss: 0.7385 - accuracy: 0.80 - 0s 146us/step - loss: 0.9761 - accuracy: 0.5669\n",
      "Epoch 191/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 0.9729 - accuracy: 0.5669\n",
      "Epoch 192/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 0.9664 - accuracy: 0.5669\n",
      "Epoch 193/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.9620 - accuracy: 0.5669\n",
      "Epoch 194/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.9574 - accuracy: 0.5827\n",
      "Epoch 195/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.9498 - accuracy: 0.5827\n",
      "Epoch 196/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9445 - accuracy: 0.5827\n",
      "Epoch 197/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9385 - accuracy: 0.5827\n",
      "Epoch 198/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.9334 - accuracy: 0.5827\n",
      "Epoch 199/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.9269 - accuracy: 0.5827\n",
      "Epoch 200/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.9208 - accuracy: 0.5984\n",
      "Epoch 201/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.9116 - accuracy: 0.6142\n",
      "Epoch 202/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.9035 - accuracy: 0.6299\n",
      "Epoch 203/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.8944 - accuracy: 0.6299\n",
      "Epoch 204/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.8835 - accuracy: 0.6614\n",
      "Epoch 205/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.8698 - accuracy: 0.6693\n",
      "Epoch 206/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.8541 - accuracy: 0.6772\n",
      "Epoch 207/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 0.8393 - accuracy: 0.6772\n",
      "Epoch 208/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 0.8278 - accuracy: 0.6929\n",
      "Epoch 209/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 0.8085 - accuracy: 0.7087\n",
      "Epoch 210/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.7948 - accuracy: 0.7087\n",
      "Epoch 211/300\n",
      "127/127 [==============================] - 0s 161us/step - loss: 0.7788 - accuracy: 0.7480\n",
      "Epoch 212/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.7629 - accuracy: 0.7402\n",
      "Epoch 213/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 0.7571 - accuracy: 0.7480\n",
      "Epoch 214/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.7444 - accuracy: 0.7559\n",
      "Epoch 215/300\n",
      "127/127 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.80 - 0s 85us/step - loss: 0.7346 - accuracy: 0.7559\n",
      "Epoch 216/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.7257 - accuracy: 0.7559\n",
      "Epoch 217/300\n",
      "127/127 [==============================] - 0s 69us/step - loss: 0.7185 - accuracy: 0.7559\n",
      "Epoch 218/300\n",
      "127/127 [==============================] - 0s 69us/step - loss: 0.7118 - accuracy: 0.7638\n",
      "Epoch 219/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.7039 - accuracy: 0.7717\n",
      "Epoch 220/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.6966 - accuracy: 0.7638\n",
      "Epoch 221/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.6909 - accuracy: 0.7717\n",
      "Epoch 222/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 0.6840 - accuracy: 0.7717\n",
      "Epoch 223/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.6774 - accuracy: 0.7717\n",
      "Epoch 224/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.6717 - accuracy: 0.7717\n",
      "Epoch 225/300\n",
      "127/127 [==============================] - 0s 77us/step - loss: 0.6659 - accuracy: 0.7717\n",
      "Epoch 226/300\n",
      "127/127 [==============================] - 0s 92us/step - loss: 0.6612 - accuracy: 0.7638\n",
      "Epoch 227/300\n",
      "127/127 [==============================] - 0s 85us/step - loss: 0.6542 - accuracy: 0.7717\n",
      "Epoch 228/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.6491 - accuracy: 0.7717\n",
      "Epoch 229/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.6433 - accuracy: 0.7717\n",
      "Epoch 230/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.6390 - accuracy: 0.7795\n",
      "Epoch 231/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.6315 - accuracy: 0.7795\n",
      "Epoch 232/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.6263 - accuracy: 0.7795\n",
      "Epoch 233/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.6234 - accuracy: 0.7795\n",
      "Epoch 234/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.6176 - accuracy: 0.7795\n",
      "Epoch 235/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 108us/step - loss: 0.6131 - accuracy: 0.7795\n",
      "Epoch 236/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.6084 - accuracy: 0.7717\n",
      "Epoch 237/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.6041 - accuracy: 0.7717\n",
      "Epoch 238/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5982 - accuracy: 0.7795\n",
      "Epoch 239/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5971 - accuracy: 0.7795\n",
      "Epoch 240/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5930 - accuracy: 0.7795\n",
      "Epoch 241/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5905 - accuracy: 0.7717\n",
      "Epoch 242/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5843 - accuracy: 0.7795\n",
      "Epoch 243/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5813 - accuracy: 0.7795\n",
      "Epoch 244/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5785 - accuracy: 0.7795\n",
      "Epoch 245/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5759 - accuracy: 0.7795\n",
      "Epoch 246/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5754 - accuracy: 0.7795\n",
      "Epoch 247/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5743 - accuracy: 0.7717\n",
      "Epoch 248/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5692 - accuracy: 0.7795\n",
      "Epoch 249/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5666 - accuracy: 0.7795\n",
      "Epoch 250/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5640 - accuracy: 0.7717\n",
      "Epoch 251/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5620 - accuracy: 0.7717\n",
      "Epoch 252/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5601 - accuracy: 0.7795\n",
      "Epoch 253/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5586 - accuracy: 0.7717\n",
      "Epoch 254/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5561 - accuracy: 0.7717\n",
      "Epoch 255/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5563 - accuracy: 0.7717\n",
      "Epoch 256/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5527 - accuracy: 0.7795\n",
      "Epoch 257/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5506 - accuracy: 0.7717\n",
      "Epoch 258/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5488 - accuracy: 0.7717\n",
      "Epoch 259/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5476 - accuracy: 0.7717\n",
      "Epoch 260/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5466 - accuracy: 0.7717\n",
      "Epoch 261/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5451 - accuracy: 0.7795\n",
      "Epoch 262/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5427 - accuracy: 0.7717\n",
      "Epoch 263/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5414 - accuracy: 0.7717\n",
      "Epoch 264/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5402 - accuracy: 0.7717\n",
      "Epoch 265/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5397 - accuracy: 0.7717\n",
      "Epoch 266/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5388 - accuracy: 0.7717\n",
      "Epoch 267/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5389 - accuracy: 0.7717\n",
      "Epoch 268/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5354 - accuracy: 0.7717\n",
      "Epoch 269/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5339 - accuracy: 0.7795\n",
      "Epoch 270/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5326 - accuracy: 0.7717\n",
      "Epoch 271/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.5319 - accuracy: 0.7717\n",
      "Epoch 272/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.5314 - accuracy: 0.7717\n",
      "Epoch 273/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5299 - accuracy: 0.7795\n",
      "Epoch 274/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5280 - accuracy: 0.7795\n",
      "Epoch 275/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5271 - accuracy: 0.7795\n",
      "Epoch 276/300\n",
      "127/127 [==============================] - 0s 131us/step - loss: 0.5268 - accuracy: 0.7717\n",
      "Epoch 277/300\n",
      "127/127 [==============================] - 0s 177us/step - loss: 0.5278 - accuracy: 0.7717\n",
      "Epoch 278/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5265 - accuracy: 0.7717\n",
      "Epoch 279/300\n",
      "127/127 [==============================] - 0s 177us/step - loss: 0.5231 - accuracy: 0.7795\n",
      "Epoch 280/300\n",
      "127/127 [==============================] - 0s 169us/step - loss: 0.5233 - accuracy: 0.7717\n",
      "Epoch 281/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 0.5224 - accuracy: 0.7795\n",
      "Epoch 282/300\n",
      "127/127 [==============================] - 0s 146us/step - loss: 0.5203 - accuracy: 0.7717\n",
      "Epoch 283/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5213 - accuracy: 0.7717\n",
      "Epoch 284/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5197 - accuracy: 0.7717\n",
      "Epoch 285/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5191 - accuracy: 0.7795\n",
      "Epoch 286/300\n",
      "127/127 [==============================] - ETA: 0s - loss: 0.6450 - accuracy: 0.60 - 0s 108us/step - loss: 0.5176 - accuracy: 0.7717\n",
      "Epoch 287/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5168 - accuracy: 0.7717\n",
      "Epoch 288/300\n",
      "127/127 [==============================] - 0s 123us/step - loss: 0.5143 - accuracy: 0.7795\n",
      "Epoch 289/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5135 - accuracy: 0.7717\n",
      "Epoch 290/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5135 - accuracy: 0.7717\n",
      "Epoch 291/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5121 - accuracy: 0.7717\n",
      "Epoch 292/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5117 - accuracy: 0.7717\n",
      "Epoch 293/300\n",
      "127/127 [==============================] - 0s 100us/step - loss: 0.5113 - accuracy: 0.7717\n",
      "Epoch 294/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5100 - accuracy: 0.7717\n",
      "Epoch 295/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5089 - accuracy: 0.7717\n",
      "Epoch 296/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5098 - accuracy: 0.7717\n",
      "Epoch 297/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5074 - accuracy: 0.7717\n",
      "Epoch 298/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5076 - accuracy: 0.7795\n",
      "Epoch 299/300\n",
      "127/127 [==============================] - 0s 115us/step - loss: 0.5062 - accuracy: 0.7717\n",
      "Epoch 300/300\n",
      "127/127 [==============================] - 0s 108us/step - loss: 0.5047 - accuracy: 0.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1e8abbad308>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train , batch_size = 10,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 2, 2, 2, 3, 0, 2, 2, 3, 2, 2, 5, 2, 2, 3, 2, 0, 5, 2,\n",
       "       2, 3, 2, 0, 0, 2, 2, 5, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model.predict_classes(sc.transform([[1100,40.1,43,45.5,12.5125,7.4165]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
